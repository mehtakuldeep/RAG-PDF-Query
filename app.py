import streamlit as st
import os
from vectordb_storage import store_pdfs_in_qdrant, query_db
from krutrim_cloud import KrutrimCloud
from dotenv import load_dotenv

load_dotenv()

st.set_page_config(page_title="RAG", layout="wide")
st.title("ğŸ“Š Your custom RAG Application")

# Sidebar: PDF Upload & Processing
st.sidebar.header("ğŸ“‚ Upload PDFs")
uploaded_files = st.sidebar.file_uploader("Upload financial reports (PDF)", type="pdf", accept_multiple_files=True)

pdf_directory = "data"
os.makedirs(pdf_directory, exist_ok=True)

if uploaded_files:
    for uploaded_file in uploaded_files:
        file_path = os.path.join(pdf_directory, uploaded_file.name)
        with open(file_path, "wb") as f:
            f.write(uploaded_file.read())
    st.sidebar.success(f"{len(uploaded_files)} PDFs uploaded successfully!")
    if st.sidebar.button("ğŸ”„ Process PDFs"):
        store_pdfs_in_qdrant(pdf_directory)
        st.sidebar.success("âœ… PDFs stored in Qdrant successfully!")

# Main Section: Querying
st.header("ğŸ” Query Financial Reports")

# User Inputs
company = st.text_input("ğŸ¢ Enter Company Name:", "bhartiairtel")
query_text = st.text_area("ğŸ“ Enter Search Query (for Vector DB):", "revenue, EBITDA, profit, net profit")
ai_query = st.text_area("ğŸ¤– Customize AI Query (for DeepSeek):", "Extract revenue and EBITDA insights in a structured table.")

if st.button("ğŸ” Get Financial Insights"):
    if not company or not query_text:
        st.warning("âš ï¸ Please enter both a company name and a query.")
    else:
        st.info(f"Processing query for '{company}'...")

        # Query Qdrant (Vector DB)
        results = query_db(company, query_text)

        if not results:
            st.warning("No relevant data found in reports.")
        else:
            # Extract relevant content from results
            document_content = "\n\n".join([res["text"] for res in results])

            # AI Processing (DeepSeek)
            client = KrutrimCloud(api_key=os.getenv("API_KEY"))
            model_name = "DeepSeek-R1"

            prompt = f"Document content:\n{document_content}\n\nUser query: {ai_query}"
            messages = [{"role": "user", "content": prompt}]

            try:
                response_stream = client.chat.completions.create(model=model_name, messages=messages)
                structured_output = response_stream.choices[0].message.content
                st.subheader("ğŸ“Š AI-Generated Financial Summary")
                st.write(structured_output)

                # (Optional) Debugging: Expandable section for raw vector DB results
                with st.expander("ğŸ› ï¸ Debug: View Raw Vector DB Matches"):
                    for res in results:
                        st.markdown(f"**Page {res['page']}** | Score: {res['score']:.2f}")
                        st.write(res["text"])
            except Exception as exc:
                st.error(f"âŒ Error fetching AI response: {exc}")